To figure out the plan, follow these steps:
0. First, infer user's real intent from the input. Determin whether he wants you to read a code file, read all code files in a folder, directly tells you the goal or gives you an existing flow.
1.1. If he wants you to read a code file, read and understand the python file's content, then resolve the whole logic into small steps;
1.2. If he wants you to read all code files in a folder, read and understand all python files' content, then resolve the whole logic into small steps;
1.3. If he gaves you the goal directly, letâ€™s first understand the goal and devise a plan to achieve the goal step by step.
1.4. If he wants you to understand an existing flow, read and understand the flow from the flow file or flow directory.
2. Each step should be implemented with only one tool. Choose the most appropriate tool for each step follow the rules: firstly check if it can be implemented with [AVAILABLE TOOLS], if not, try to use tool in [PYTHON TOOLS] for the step if you need to access the internet, do math calculation, read/write files, process data or use algorithm. And you should also provide the implementation code clearly in detail. If you do not know how to implement the tool using python clearly, use the tool in [LLM TOOLS] if the step is suit for llm to accomplish.
3. Create a flow in YAML format to represent the plan.
4. A flow has 'input' available in flow variables by default.
5. flow's outputs must reference one of the node's output.
6. Always output valid YAML that can be parsed by a YAML parser.

All flows take the form of:
inputs:
  url:
    type: string
    default: https://www.microsoft.com
outputs:
  category:
    type: string
    reference: ${convert_to_dict.output.category}
  evidence:
    type: string
    reference: ${convert_to_dict.output.evidence}
nodes:
- name: fetch_text_content_from_url
  type: python
  source:
    type: code
    path: fetch_text_content_from_url.py
  inputs:
    url: ${inputs.url}
- name: summarize_text_content
  type: llm
  source:
    type: code
    path: summarize_text_content.jinja2
  inputs:
    deployment_name: ''
    max_tokens: '128'
    temperature: '0.2'
    text: ${fetch_text_content_from_url.output}
  provider: AzureOpenAI
  connection: ''
  api: completion
- name: prepare_examples
  type: python
  source:
    type: code
    path: prepare_examples.py
  inputs: {}
- name: classify_with_llm
  type: llm
  source:
    type: code
    path: classify_with_llm.jinja2
  inputs:
    deployment_name: ''
    max_tokens: '128'
    temperature: '0.2'
    url: ${inputs.url}
    text_content: ${summarize_text_content.output}
    examples: ${prepare_examples.output}
  provider: AzureOpenAI
  connection: ''
  api: completion
- name: convert_to_dict
  type: python
  source:
    type: code
    path: convert_to_dict.py
  inputs:
    input_str: ${classify_with_llm.output}

When create flow, folow below rules:
1. A node has one or more named inputs and a single 'output' which are all strings. One node can only use one tool.
2. To save an 'output' from a node named node_1, and pass into a future node, use ${node_1.output}
3. To save an 'output' from a node named node_2, and return as part of a flow output, use ${node_2.output}
4. flow's outputs must comes from one of the node's output, define it clearly in flow yaml, for example, if a flow output comes from node_3, reference it as "reference: ${node_3.output}"
5. Different nodes can consume the output from the same upstream node
6. If node_1 pass its output to node_2, it means node_1 is the preorder node of node_2
7. If the node uses llm tool, you should format a prompt template as the llm's input, to reference the input of the node, for example input_1, use {{ "{% raw %}" }}{{ "{{" }} input_1{{ "}}" }}{{ "{% endraw %}" }} to reference in the template. Do not forget to include provider, connection and api for llm node in yaml
8. The flow's input and output can only have one of the types in ['int', 'double', 'bool', 'string', 'list', 'object']
9. Do not forget to add 'from promptflow import tool' in each python tool's impemenation
10. Do not forget to declar the parametr's type for each python tool's implementation function
11. Do not use TODO in the python function tool's implementation, give the implementation code in detail explicitly
12. Do not generate prompt for python tool step
13. For each node that use python tool, you should also give the impemenation of the python function in json string format. Remember that you are a good python programmer, you should implement the python function in detail, and you will leverage any public python packages which can be found from pypi if needed
14. For each node that uses llm tool, use natual language to tell llm what do you want clearly and in detail and treat it as the llm node's prompt. you can add extra input to receive pre-order nodes's output and use it in the prompt for example, you want to use the output of node_1, you can define an extra input named 'upper_text', and use {{ "{% raw %}" }}{{ "{{" }} upper_text {{ "}}" }}{{ "{% endraw %}" }} to reference in the prompt.
15. All python functions should be decorated with @tool decorator. And the tool decorator is imported from promptflow package, for example: {"convert_to_dict": "import json\nfrom promptflow import tool\n\n@tool\ndef convert_to_dict(input_str: str):\n    return json.dumps(input_str)\n"}
16. Format all python functions in a list of dictionary, use the function's name as the key and the json string as the value.
17. Format all llm node's prompts in a list of dictionary, use the node's name as the key and the prompt as the value.
18. Review all python functions, fix the grammer error and optimize the impemenation
19. list all extra python packages you used in the python functions
20. list flow's inputs' and outputs' schema

Please format your answer into seven parts: flow_yaml, explaination, python_functions, prompts, requirements, flow_inputs_schema, flow_outputs_schema

Pay attention:
python_functions should be a list of dictionary, use the function's name as the key and the json string as the value.
prompts should be a list of dictionary, use the node's name as the key and the prompt as the value.
flow_inputs_schema should be a list of dictionary, use the input's name as the key and the schema as the value.
flow_outputs_schema should be a list of dictionary, use the output's name as the key and the schema as the value.

for example:
flow_yaml:
inputs:\n  url:\n    type: string\n    default: https://www.microsoft.com\noutputs:\n  category:\n    type: string\n    reference: ${convert_to_dict.output.category}\n  evidence:\n    type: string\n    reference: ${convert_to_dict.output.evidence}\nnodes:\n- name: fetch_text_content_from_url\n  type: python\n  source:\n    type: code\n    path: fetch_text_content_from_url.py\n  inputs:\n    url: ${inputs.url}\n- name: summarize_text_content\n  type: llm\n  source:\n    type: code\n    path: summarize_text_content.jinja2\n  inputs:\n    deployment_name: \'\'\n    max_tokens: \'128\'\n    temperature: \'0.2\'\n    text: ${fetch_text_content_from_url.output}\n  provider: AzureOpenAI\n  connection: \'\'\n  api: completion\n- name: prepare_examples\n  type: python\n  source:\n    type: code\n    path: prepare_examples.py\n  inputs: {}\n- name: classify_with_llm\n  type: llm\n  source:\n    type: code\n    path: classify_with_llm.jinja2\n  inputs:\n    deployment_name: \'\'\n    max_tokens: \'128\'\n    temperature: \'0.2\'\n    url: ${inputs.url}\n    text_content: ${summarize_text_content.output}\n    examples: ${prepare_examples.output}\n  provider: AzureOpenAI\n  connection: \'\'\n  api: completion\n- name: convert_to_dict\n  type: python\n  source:\n    type: code\n    path: convert_to_dict.py\n  inputs:\n    input_str: ${classify_with_llm.output}

explaination:
given a web url, we use python tool to implement a function to fetch the content for the website, then we use a llm tool to generate summary of the fetched content. Also we use a python tool to prepare some example on how to classify content to different categories.
at last, we use a llm tool to classify the website according to the content we fetched and the samples we prepared and convert the result to a dict.

python_functions:
[
  {"convert_to_dict": "import json\nfrom promptflow import tool\n\n@tool\ndef convert_to_dict(input_str: str):\n    return json.dumps(input_str)\n"}
  {"prepare_examples" "from promptflow import tool\n@tool\ndef prepare_examples():\n    return [\n        {\n            \"url\": \"https://play.google.com/store/apps/details?id=com.spotify.music\",\n            \"text_content\": \"Spotify is a free music and podcast streaming app with millions of songs, albums, and \"\n                            \"original podcasts. It also offers audiobooks, so users can enjoy thousands of stories. \"\n                            \"It has a variety of features such as creating and sharing music playlists, discovering \"\n                            \"new music, and listening to popular and exclusive podcasts. It also has a Premium \"\n                            \"subscription option which allows users to download and listen offline, and access \"\n                            \"ad-free music. It is available on all devices and has a variety of genres and artists \"\n                            \"to choose from.\",\n            \"category\": \"App\",\n            \"evidence\": \"Both\"\n        },\n        {\n            \"url\": \"https://www.youtube.com/channel/UC_x5XG1OV2P6uZZ5FSM9Ttw\",\n            \"text_content\": \"NFL Sunday Ticket is a service offered by Google LLC that allows users to watch NFL \"\n                            \"games on YouTube. It is available in 2023 and is subject to the terms and privacy policy \"\n                            \"of Google LLC. It is also subject to YouTube's terms of use and any applicable laws.\",\n            \"category\": \"Channel\",\n            \"evidence\": \"URL\"\n        }\n    ]"}
  {"fetch_text_content_from_url": "from promptflow import tool\nimport requests\nimport bs4\n\n\n@tool\ndef fetch_text_content_from_url(url: str):\n    # Send a request to the URL\n    try:\n        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n                                 \"Chrome/113.0.0.0 Safari/537.36 Edg/113.0.1774.35\"}\n        response = requests.get(url, headers=headers)\n        if response.status_code == 200:\n            # Parse the HTML content using BeautifulSoup\n            soup = bs4.BeautifulSoup(response.text, 'html.parser')\n            soup.prettify()\n            return soup.get_text()[:2000]\n        else:\n            msg = f\"Get url failed with status code {response.status_code}.\nURL: {url}\nResponse: \" \\n                  f\"{response.text[:100]}\"\n            print(msg)\n            return \"No available content\"\n    except Exception as e:\n        print(\"Get url failed with error: {}\".format(e))\n        return \"No available content\""}
]

prompts:
[
  {"summarize_text_content": "Please summarize the following text in one paragraph. 100 words.\nDo not add any information that is not in the text.\n\nText: {{ "{% raw %}" }}{{ "{{" }} text {{ "}}" }}{{ "{% endraw %}" }}\nSummary: "},
  {"classify_with_llm": "Your task is to classify a given url into one of the following types:\nMovie, App, Academic, Channel, Profile, PDF or None based on the text content information.\nThe classification will be based on the url, the webpage text content summary, or both.\n\nHere are a few examples:\n{% raw %}{% for ex in examples %}{% endraw %}\nURL: {{ "{% raw %}" }}{{ "{{" }} ex.url {{ "}}" }}{{ "{% endraw %}" }}\nText content: {{ "{% raw %}" }}{{ "{{" }} ex.text_content {{ "}}" }}{{ "{% endraw %}" }}\nOUTPUT:\n{\"category\": \"{{ "{% raw %}" }}{{ "{{" }} ex.category{{ "}}" }}{{ "{% endraw %}" }}\", \"evidence\": \"{{ "{% raw %}" }}{{ "{{" }} ex.evidence{{ "}}" }}{{ "{% endraw %}" }}\"}\n\n{% raw %}{% endfor %}{% endraw %}\n\nFor a given URL : {{ "{% raw %}" }}{{ "{{" }} url {{ "}}" }}{{ "{% endraw %}" }}, and text content: {{ "{% raw %}" }}{{ "{{" }} text_content {{ "}}" }}{{ "{% endraw %}" }}.\nClassify above url to complete the category and indicate evidence.\nOUTPUT:"}
]

requirements:
"beautifulsoup4\n"

flow_inputs_schema:
[{"url": {"type": "string", "description": "The url of the website to be classified."}}]

flow_outputs_schema:
[
  {"category": {"type": "string", "description": "The category of the website."}},
  {"evidence": {"type": "string", "description": "The evidence of the classification."}}
]